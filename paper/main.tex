%% LyX 2.2.0rc1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{url}
\usepackage{amsmath}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage[utf8]{inputenc}
%\usepackage{fontspec} % This line only for XeLaTeX and LuaLaTeX
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{pgfplots}
%\usetikzlibrary{pgfplots.groupplots}

\makeatother

\usepackage{babel}
\begin{document}

\title{Branching Gaussian processes}

\author{Alexis Boukouvalas, James Hensman, Magnus Rattray}

\date{January 25, 2017}

\maketitle
\global\long\def\N{\mathcal{N}}
\global\long\def\Y{\mathcal{Y}}
\global\long\def\Z{\mathcal{Z}}
\global\long\def\F{f}
\global\long\def\so{\mathcal{\sigma}}
\global\long\def\PhiM{\mathcal{\Phi}}
\global\long\def\m{M}
\global\long\def\n{N}
\global\long\def\X{X}
\global\long\def\t{t}

\tableofcontents{}

\section{Introduction}

Single-cell gene expression data can be used to uncover cellular progression
through different states of a temporal transformation, e.g. during
development, differentiation or disease. A single pseudotime parameter
can be assigned to each cell to represent its cellular state. We propose
a non-linear model to estimate the branching tree structure. The model
provides a log likelihood ratio estimate of the evidence for branching
and a posterior estimate of the most likely branching location as
well as a confidence interval.

\cite{yang2016inferring} developed a tractable GP model for the identification
of a single perturbation point. They define a novel kernel that constrains
two functions to intersect at a single point. The bifurcation point
is identified by numerically approximating the posterior and selecting
a point estimate. The model is used to identify when a gene becomes
differentially expressed in time course gene expression data under
a control and perturbed condition. In their approach all data points
have been labelled with the branch that generated them. The ordering
of time points is also assumed known and fixed.

We extend their approach by removing the labelling assumption and
include additional steps in the workflow in order to apply it to single-cell
data. These include an estimate of the pseudotime using \cite{monocle2}.

\section{Synthetic data:}

We sample from a branching GP. Samples where branches cross after
bifurcation point are rejected to avoid penalizing linear methods
like MFA. We fit the branching GP model as described in appendix \ref{sec:Workflow}. 

We evaluate three methods, the mixture of factors analysers \cite{mfa},
the BEAM approach \cite{monocle2} and the branching GP model. The
synthetic scenarios are summarized in Table \ref{tab:Scenarios}. 

The log likelihood ratio of the branching GP can be used to rank the
evidence of ranking for each. Similar measures exist for the MFA and
BEAM method. We compare the three methods on their ability to discriminate
branching genes from non-branching genes. The metric we have selected
for this comparison is the area under the curve which has been classically
used to evaluate classification models. In Table \ref{tab:ScenariosAUC},
the branching GP achieves consistently good performance whilst MFA
performance varies significantly. The BEAM method is unable to discriminate
well. 

\begin{table}
\caption{Scenarios used to generate synthetic data. The specification of each
scenario includes the number of genes branching at each location.
A branching location of 1.1 refers to a non-branching gene. \label{tab:Scenarios}}

\centering{}%
\begin{tabular}{|c|c|c|}
\hline 
Scenario & Branching & Description\tabularnewline
\hline 
\hline 
0 & {[}0.2, 20{]}, {[}1.1, 20{]} & Single branching\tabularnewline
\hline 
1 & {[}0.2, 20{]}, {[}0.6, 20{]} & All genes branching\tabularnewline
\hline 
2 & {[}0.2, 15{]}, {[}0.6, 15{]}, {[}1.1, 10{]} & Multiple branching points\tabularnewline
\hline 
3 & {[}0.2, 15{]}, {[}0.6, 15{]}, {[}1.1, 10{]} & Short lengthscale \tabularnewline
\hline 
4 & {[}0.1, 3{]}, {[}0.7, 27{]}, {[}1.1, 10{]} & Majority of late branching genes\tabularnewline
\hline 
5 & {[}0.1, 5{]}, {[}0.3, 5{]}, {[}0.5, 5{]}, {[}0.7, 5{]}, {[}1.1, 20{]} & Many branching locations\tabularnewline
\hline 
6 & {[}0.2, 20{]}, {[}1.1, 20{]} & High branching variance\tabularnewline
\hline 
\end{tabular}
\end{table}

\begin{table}
\caption{Evaluating the discriminative ability of each method to identify branching
genes. The area under the curve (AUC) metric is reported for each
scenario and we compare the mixture of factor analysers, BEAM and
branching GP methods.\label{tab:ScenariosAUC}}

\centering{}%
\begin{tabular}{|c|c|c|c|}
\hline 
Scenario & MFA & BEAM & BGP\tabularnewline
\hline 
\hline 
0 & 0.84 & 0.6 & 0.95\tabularnewline
\hline 
2 & 0.22 & 0.59 & 0.9\tabularnewline
\hline 
3 & 0.66 & 0.59 & 0.95\tabularnewline
\hline 
4 & 0.81 & 0.66 & 0.92\tabularnewline
\hline 
5 & 0.91 & 0.52 & 0.98\tabularnewline
\hline 
6 & 0.92 & 0.63 & 0.83\tabularnewline
\hline 
\end{tabular}
\end{table}

\begin{figure}[htbp!] 
% syntheticPlot.py sid=3
\centering
\resizebox{0.5\textwidth}{!}
{\input{figs/synthetic_MonoclePseudotime_scen3.tex}}
\caption{Synthetic data, scenario 3. Monocle2 latent space and pseudotime estimation. The global branching estimated by Monocle is also shown. The green circle denotes the starting cell.}
\end{figure}

\begin{figure}[htbp!] 
% scen3 notebook
\centering
\mbox{

\subfigure[Log likelihood. Branching (blue) vs non-branching (red)]
{\resizebox{0.5\textwidth}{!}
{\input{figs/synthetic_loglikelihoodSummary_scen3.tex}}}

\subfigure[Posterior summary]
{\resizebox{0.5\textwidth}{!}
{\input{figs/synthetic_posteriorSummary_scen3.tex}}}
}
\caption{Synthetic data, scenario 3. Posterior summary}
\end{figure}

\begin{figure}[htbp!] 
% scen3 notebook
\centering
\mbox{
\subfigure[Early]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_sample0.tex}}}\subfigure[Late]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_sample15.tex}}}
\subfigure[No branching]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_sample30.tex}}}
}
\mbox{
\subfigure[GP Early]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_gexpr_0.tex}}}
\subfigure[GP Late]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_gexpr_15.tex}}}
\subfigure[GP No branching]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_gexpr_30.tex}}}
}
\mbox{
\subfigure[Objective Early]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_obj_0.tex}}}
\subfigure[Objective Late]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_obj_15.tex}}}
\subfigure[Objective No branching]
{\resizebox{0.3\textwidth}{!}{\input{figs/synthetic_scen3_obj_30.tex}}}
}
\caption{Synthetic data, scenario 3. Samples, model predictions and objective function shown for scenario 3 representing early, late and no branching. The data shown in the GP plots do not match the samples due to the estimation of pseudotime by Monocle.}
\end{figure}

\section{Application to droplet single-cell RNA-seq data}

Our aim is to perform a gene by gene application of the probabilistic
branching model to see if we can differentiate early and late branching
genes. Our analysis is on the postmitotic normalised data. The data
consists of 3299 cells and 2405 genes.

Firstly we apply the Monocle \cite{monocle2} method to get a global
pseudotime, branching estimation as well as label allocation. Monocle
identifies lots of segments and we simplify the problem to two main
branches. We have selected the starting cell by looking at the dimension
reduction visualization. However this should be further validated,
perhaps by looking at known marker genes. We then score genes by difference
in the t-statistic for branch end states. The full pipeline is described
in appendix \ref{sec:Workflow}.

We apply the branching Gaussian process (GP) model to identify early
and late branching genes. We apply the model on the top 500 genes
from the previous step. There are multiple source of errors such as
dropout and misclassification of cells by the global branch estimation.
We therefore treat the labels as uncertain and the branching GP model
may reassign cells to the other branch. This also allows for more
flexible noise modeling including dropout without the need for a complex
probabilistic noise model.


\begin{figure}[htbp!] 
\centering
\resizebox{0.9\textwidth}{!}{\input{figs/dropseq_MonoclePseudotime.tex}}
\caption{Dropseq data. Monocle reduce spaced coloured by pseudotime.}
\end{figure}


\begin{figure}[htbp!] 
\centering
\resizebox{0.9\textwidth}{!}{\input{figs/dropseq_loglikelihoodSpectrum.tex}}
\caption{Dropseq data. Log-likelihood rank of top 500 genes.}
\end{figure}

\begin{figure}[htbp!] 
\centering
\resizebox{0.9\textwidth}{!}{\input{figs/dropseq_posteriorSummary.tex}}
\caption{Dropseq data. Posterior branching summary.}
\end{figure}

\begin{figure}[htbp!] 
\centering
\resizebox{0.9\textwidth}{!}{\input{figs/dropseq_loglikelihoodSummary.tex}}
\caption{Dropseq data. Log-likelihood ratio}
\end{figure}

%Early 'Nhs', 'Tcf4', 'Meis2' Med  'Maf', 'Ina', 'Gria1', 'Digap1' Late 'Rarb', 'Sybu'
\pgfplotsset{ticks=none}  % do not show axis

\begin{figure}[htbp!] 
\centering
\mbox{
\subfigure[Nhs] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_gexpr_Nhs.tex}}}
\subfigure[Nhs] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_obj_Nhs.tex}}}
\subfigure[Nhs] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_ma_Nhs.tex}}}
}
\mbox{
\subfigure[Tcf4]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_gexpr_Tcf4.tex}}}
\subfigure[Tcf4]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_obj_Tcf4.tex}}}
\subfigure[Tcf4]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_ma_Tcf4.tex}}}
}
\mbox{
\subfigure[Meis2]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_gexpr_Meis2.tex}}}
\subfigure[Meis2]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_obj_Meis2.tex}}}
\subfigure[Meis2]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_ma_Meis2.tex}}}
}
\caption{Dropseq data. Early branching genes}
\label{figs.dropseqEarly}
\end{figure}

\begin{figure}[htbp!] 
\centering
\mbox{
\subfigure[Maf] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_gexpr_Maf.tex}}}
\subfigure[Maf] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_obj_Maf.tex}}}
\subfigure[Maf] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_ma_Maf.tex}}}
}
\mbox{
\subfigure[Ina]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_gexpr_Ina.tex}}}
\subfigure[Ina]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_obj_Ina.tex}}}
\subfigure[Ina]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_ma_Ina.tex}}}
}
\mbox{
\subfigure[Gria1]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_gexpr_Gria1.tex}}}
\subfigure[Gria1]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_obj_Gria1.tex}}}
\subfigure[Gria1]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_ma_Gria1.tex}}}
}
\caption{Dropseq data. Medium branching genes}
\label{figs.dropseqMed}
\end{figure}

\begin{figure}[htbp!] 
\centering
\mbox{
\subfigure[Rarb] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_gexpr_Rarb.tex}}}
\subfigure[Rarb] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_obj_Rarb.tex}}}
\subfigure[Rarb] {\resizebox{0.3\textwidth}{!} {\input{figs/dropseq_ma_Rarb.tex}}}
}
\mbox{
\subfigure[Sybu]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_gexpr_Sybu.tex}}}
\subfigure[Sybu]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_obj_Sybu.tex}}}
\subfigure[Sybu]{\resizebox{0.3\textwidth}{!}{\input{figs/dropseq_ma_Sybu.tex}}}
}
\caption{Dropseq data. Late branching genes}
\label{figs.dropseqLate}
\end{figure}

\section{Theory}

We can define a kernel that describes two functions crossing at a
single point \cite{yang2016inferring}\footnote{This result is re-derived in Appendix \ref{sec:TwoFunctionsCrossing}. }.
In this work however we cannot assume the data points are labelled
with the function the generated them. \cite{Lazaro-Gredilla2012,lazaro2014gaussian}
define this is the data association problem and propose the overlapping
mixture of GPs to address it.

However using the OMG model to identify branches in pseudotime would
be wasteful since we the functions are not independent as they are
constrained to intersect at the branching point. Rather we believe
there is underlying tree structure where branches occur at bifurcation
points. 

Let the likelihood be $p\left(\Y|\Z,\X\right)$ where $\Z$ is the
$\n\times\m$ binary indicator matrix, $\X$ is the pseudotime ($\n\times1$)
and $\Y$ the data ($\n\times D$). $\m\gg\n$ is the number of possible
allocations for all points; for example if we have a single branching
point whose location is unknown each point has 3 possible allocations
and therefore $\m=3\times\n$.

We assume a Gaussian error model with common observation noise $\so^{2}$,
independent dimensions $d$ and we place a GP prior on the latent
function values 

\begin{equation}
p\left(\F\left|\X\right.\right)=\mathcal{GP}\left(0,K\left(\X\right)\right)
\end{equation}

The likelihood can then be written as 

\begin{equation}
p\left(\Y|\Z,\X\right)=\int_{f}p\left(\F\left|\X\right.\right)\prod_{d}\N\left(\Y_{d}\left|\Z\F,\so^{2}I_{N}\right.\right)
\end{equation}

where $f$ is the $\m\times1$ vector of latent noise free function
values. Let $\X_{*}$ the expanded version of $\X$ where is entry
of the latter is replicated $\m$ times resulting in a $\left(\n\times\m\right)\times1$
vector. Then the marginal likelihood is
\begin{equation}
p\left(\Y|\Z,\X\right)=\prod_{d}\N\left(\Y_{d}\left|0,vec\left(Z\right)K\left(\X_{*}\right)+\so^{2}I_{N}\right.\right)
\end{equation}


\section{Extensions}

Extending to multiple branching points is possible by discretizing
the pseudotime space and trying all possible trees. Although computationally
expensive, this is not intractable as pseudotime is a one-dimensional
space. The number of permutations governed by Catalan number \footnote{See \url{https://en.wikipedia.org/wiki/Catalan_number}.}.

We also constrain tree to be rooted at start of pseudotime, that is
the branching grows with pseudo-time. In theory we could allow for
a full graph in pseudo-time but prefer the simpler approach as it
is also simpler to elicit the number of leafs in a binary tree. We
could also use K-trees, where at each bifurcation point we could have
$K>2$ branches but again for reasons of simplicity we select the
binary tree, that is $K=2$.

\appendix

\section{Joint distribution of two functions constrained at a point\label{sec:TwoFunctionsCrossing}}

We prove the result in Section 2.3 \cite{jingyang2015}. The following
Gaussian Identity will be useful (Section 2.3.3, page 93, Equations
2.113-2.117 \cite{bishopBook}). Let $p\left(x\right)=N\left(x\vert\mu,\Lambda^{-1}\right)$
and $p\left(y|x\right)=N\left(y\vert Ax+b,L^{-1}\right)$ then we
have 
\begin{equation}
p\left(y\right)=N\left(y|A\mu+b,L^{-1}+A\Lambda^{-1}A^{T}\right)\label{eq:bishopresult}
\end{equation}
.

As \cite{jingyang2015} discuss the predictive mean of a GP conditional
on a single point $\left(u,x_{p}\right)$ is $\mu\left(X\right)=\frac{k_{X}}{k_{p}}u$
and $C\left(X,X\right)=K_{X}-\frac{k_{X}k_{X}^{T}}{k_{p}}$where we
denote $K_{X}=K\left(X,X\right)$ the $N\times N$ test covariance,
$k_{X}=K\left(X,x_{p}\right)$ the $N\times1$ train-test matrix and
$k_{p}=K\left(x_{p},x_{p}\right)$ the scalar. Note that the latter
does not depend on the kernel length scale but only on the variance
terms (process+nugget).

We now integrate out the latent response value at the bifurcation
point by assuming 
\begin{equation}
p(u)=N\left(u\left|0,k_{p}\right.\right)\label{eq:prioru}
\end{equation}
 We also assume conditional independence between the responses $f$
and $g$, that is $p\left(f,g|u\right)=p\left(f|u\right)p\left(g|u\right)$.
Then we have 
\begin{equation}
p\left(f,g\right)=\int p\left(f,g|u\right)p\left(u\right)du\label{eq:intu}
\end{equation}
If we observe process $f$ at points $X$ and process $g$ at point
$Z$ then we have 
\begin{eqnarray}
p\left(f\left(X\right),g\left(Z\right)|u\right) & = & \N\left(\begin{pmatrix}f\\
g
\end{pmatrix}\left|\begin{pmatrix}\mu\left(X\right)\\
\mu\left(Z\right)
\end{pmatrix},\begin{pmatrix}C\left(X,X\right) & 0\\
0 & C\left(Z,Z\right)
\end{pmatrix}\right.\right)\label{eq:fgulikelihood}\\
 & = & \N\left(\begin{pmatrix}f\\
g
\end{pmatrix}\left|\begin{pmatrix}\frac{k_{X}}{k_{p}}\\
\frac{k_{Z}}{k_{p}}
\end{pmatrix}u,\begin{pmatrix}C\left(X,X\right) & 0\\
0 & C\left(Z,Z\right)
\end{pmatrix}\right.\right)
\end{eqnarray}
 an $2N\times2N$ matrix where $C\left(.,.\right)$ as defined previously.

We can now use the result in Equation (\ref{eq:bishopresult}) to
solve the integral in Equation (\ref{eq:intu}). 

\begin{eqnarray*}
p\left(f\left(X\right),g\left(Z\right)\right) & = & \N\left(\begin{pmatrix}f\\
g
\end{pmatrix}\left|0,\begin{pmatrix}C\left(X,X\right) & 0\\
0 & C\left(Z,Z\right)
\end{pmatrix}+\begin{pmatrix}\frac{k_{X}}{k_{p}}\\
\frac{k_{Z}}{k_{p}}
\end{pmatrix}k_{p}\begin{pmatrix}\frac{k_{X}}{k_{p}}\\
\frac{k_{Z}}{k_{p}}
\end{pmatrix}^{T}\right.\right)\\
 & = & \N\left(\begin{pmatrix}f\\
g
\end{pmatrix}\left|0,\begin{pmatrix}C\left(X,X\right) & 0\\
0 & C\left(Z,Z\right)
\end{pmatrix}+\begin{pmatrix}\frac{k_{X}k_{X}^{T}}{k_{p}} & \frac{k_{X}k_{Z}^{T}}{k_{p}}\\
\frac{k_{Z}k_{X}^{T}}{k_{p}} & \frac{k_{Z}k_{Z}^{T}}{k_{p}}
\end{pmatrix}\right.\right)\\
 & = & \N\left(\begin{pmatrix}f\\
g
\end{pmatrix}\left|0,\begin{pmatrix}K_{X} & \frac{k_{X}k_{Z}^{T}}{k_{p}}\\
\frac{k_{Z}k_{X}^{T}}{k_{p}} & K_{Z}
\end{pmatrix}\right.\right)
\end{eqnarray*}

which proves the result in Equation 6 of \cite{jingyang2015}.

\section{Crossing kernel}

The crossing kernel is

\[
p\left(f\left(X\right),g\left(Z\right)\right)=\int p\left(f|u\right)p\left(g|u\right)p\left(u\right)du=\N\left(\begin{pmatrix}f\\
g
\end{pmatrix}\left|0,\begin{pmatrix}K_{X} & k_{X}k_{p}^{-1}k_{Z}^{T}\\
k_{Z}k_{p}^{-1}k_{X}^{T} & K_{Z}
\end{pmatrix}\right.\right)
\]
 where $k_{X}=K\left(X,x_{p}\right)$ the $N\times1$ train-test matrix
and $k_{p}=K\left(x_{p},x_{p}\right)$ the branching kernel.

\section{Efficient implementation using TensorFlow.\label{sec:Tensorflow}}

We have implemented our approach in the GPflow framework \cite{GPflow2016}
which leverages efficient computation using Tensorflow \cite{tensorflow2015-whitepaper}.

In the latter we build an executable graph using python that can then
be executed across many CPU/GPUs or in a distributed environment.
The goal is to easily transition from research prototype to production.

\section{Workflow\label{sec:Workflow}}

The workflow we use in details is given below.
\begin{enumerate}
\item Apply Monocle 2 method to get a global pseudotime, branching estimation
as well as label allocation. Monocle identifies lots of 'segments'
and we simplify the problem to two main branches. 
\item Rank genes by median distance or t-statistic between the tips of the
branches. 
\item Normalise the data: $y=(y-y_{min})/(np.percentile(y,99)-y_{min})$.
This reduces the effective range of the gene expression reducing the
effect of outliers and noise. 
\item Smooth the data using a running mean for each each branch. This avoids
more complex than Gaussian noise models. 
\item Initialise allocation matrix Phi using Monocle. If branch 1, branch
2 and 3 {[}0.5 0.5{]}, if branch 2 {[}0.75 0.25{]}. 
\item Stretch branches to be same length, as in Monocle 2. 
\item Estimate a branching GP using the Monocle 2 branching point. We estimate
the kernel hyperparameters as well. 
\item We are using a Matern 3/2 kernel - perhaps a rougher kernel would
be more appropriate. 
\item Estimate hyperparameters at global branching location. 
\item Perform a grid search on other branching locations keeping hyperparameters
fixed. 
\end{enumerate}
\bibliographystyle{plain}
\bibliography{bibFile}

\end{document}
